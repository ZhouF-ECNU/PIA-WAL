{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import auc,roc_curve, precision_recall_curve, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,f1_score\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "# from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
    "# from tensorflow.keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import random\n",
    "random_seed = 42\n",
    "MAX_INT = np.iinfo(np.int32).max\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviation_loss(y_true, y_pred):\n",
    "    '''\n",
    "    z-score-based deviation loss\n",
    "    '''    \n",
    "    confidence_margin = 5.0     ### 修改了这里\n",
    "    inlier_loss = K.abs(y_pred) \n",
    "    outlier_loss = K.maximum(confidence_margin - y_pred, 0.)\n",
    "    return (1 - y_true) * inlier_loss + y_true * outlier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(input_shape):\n",
    "    '''\n",
    "    network architecture with one hidden layer\n",
    "    '''\n",
    "    x_input = Input(shape=input_shape)\n",
    "    intermediate = Dense(20, activation='relu', \n",
    "                kernel_regularizer=regularizers.l2(0.01), name = 'hl1')(x_input)## 修改了这里\n",
    "    intermediate = Dense(1, activation='linear', name = 'score')(intermediate)    \n",
    "    return Model(x_input, intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_score_learner(input_shape, lr):\n",
    "    '''\n",
    "    construct the deviation network-based detection model\n",
    "    '''\n",
    "    model = network(input_shape)\n",
    "    rms = RMSprop(clipnorm = 1.,learning_rate = lr) ### 修改了这里\n",
    "    model.compile(loss = deviation_loss, optimizer = rms)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weight_predict(model_name, input_shape, x_test):\n",
    "    '''\n",
    "    load the saved weights to make predictions\n",
    "    '''\n",
    "    model = deviation_network(input_shape)\n",
    "    model.load_weights(model_name)\n",
    "    scoring_network = Model(inputs=model.input, outputs=model.output)    \n",
    "    \n",
    "    scores = scoring_network.predict(x_test)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小为(162079, 40)\n",
      "Counter({0: 158441, 1: 3638})\n",
      "测试集大小为(40520, 40)\n",
      "Counter({0: 39611, 1: 909})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Test_data = pd.read_csv('data/celeba/x_test.csv',index_col= 0)\n",
    "Train_data = pd.read_csv('data/celeba/x_train.csv',index_col= 0)\n",
    "\n",
    "print('训练集大小为{}'.format(Train_data.shape))\n",
    "print(Counter(Train_data.loc[:,'class']))\n",
    "\n",
    "\n",
    "print('测试集大小为{}'.format(Test_data.shape))\n",
    "print(Counter(Test_data.loc[:,'class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158441, 39)\n",
      "(7, 39)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 39611, 1: 909})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_normal = Train_data[(Train_data['class'] == 0)].reset_index(drop=True).iloc[:,:-1].values\n",
    "x_train_normal = x_train_normal.astype(np.float32)\n",
    "x_train_normal[np.isnan(x_train_normal)] = 0\n",
    "print(x_train_normal.shape)\n",
    "\n",
    "\n",
    "x_train_vandal = Train_data[(Train_data['class'] == 1)].sample(n=70,random_state=42).reset_index(drop=True).iloc[:,:-1].values\n",
    "x_train_vandal = x_train_vandal.astype(np.float32)\n",
    "x_train_vandal[np.isnan(x_train_vandal)] = 0\n",
    "print(x_train_vandal.shape)\n",
    "\n",
    "\n",
    "x_test = Test_data.iloc[:,:-1].values\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_test[np.isnan(x_test)] = 0\n",
    "y_test = Test_data.iloc[:,-1].values\n",
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_shuffle_uspv(X, labels, weights):\n",
    "#     print(X.shape,labels.shape,weights.shape)\n",
    "    n_samples = len(X)\n",
    "    s = np.arange(n_samples)\n",
    "    np.random.shuffle(s)\n",
    "    return np.array(X[s]), labels[s], weights[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Dataset iterator\n",
    "def inf_normal_train_gen(BATCH_SIZE,normal_data_):\n",
    "    n_samples = len(normal_data_)\n",
    "    s = np.arange(n_samples)\n",
    "    while True:\n",
    "        np.random.shuffle(s)\n",
    "        normal_data_ = np.array(normal_data_[s])\n",
    "        for i in range(int(len(normal_data_)/BATCH_SIZE)):\n",
    "            yield normal_data_[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "            \n",
    "# Dataset iterator\n",
    "def inf_vandal_train_gen(BATCH_SIZE,vandal_data_):\n",
    "    n_samples = len(vandal_data_)\n",
    "    s = np.arange(n_samples)\n",
    "    while True:\n",
    "        np.random.shuffle(s)\n",
    "        vandal_data_ = np.array(vandal_data_[s])\n",
    "        dataset = []\n",
    "        for i in range(BATCH_SIZE):\n",
    "            dataset.append(random.choice(vandal_data_))\n",
    "        dataset = np.array(dataset, dtype='float32')\n",
    "        yield dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_hard_normal_weight_train_gen(BATCH_SIZE,normal_scores,normal_data_):\n",
    "    indicator = np.sign(np.abs(normal_scores) - 5.0)\n",
    "    condition = np.greater(indicator, np.zeros_like(indicator))\n",
    "    mask_tar = np.where(condition, np.zeros_like(indicator),np.tanh(np.abs(normal_scores)))\n",
    "    dataset = []\n",
    "    weights = []\n",
    "#     mean_scores = np.mean(mask_tar)\n",
    "    mean_scores = 0\n",
    "    candicate_idxs = np.where(mask_tar>mean_scores)[0]\n",
    "    while True:\n",
    "        for idx in candicate_idxs:\n",
    "            tar = random.uniform(mean_scores, 1.0) ### 修改了这里\n",
    "            if mask_tar[idx]>tar:\n",
    "                dataset.append(normal_data_[idx])\n",
    "                weights.append(np.abs(normal_scores[idx]))\n",
    "            if len(dataset)==BATCH_SIZE:\n",
    "                dataset = np.array(dataset, dtype='float32')\n",
    "                weights = np.array(weights, dtype='float32')\n",
    "#                 print(idx)\n",
    "                yield dataset,weights\n",
    "                dataset = []\n",
    "                weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self, input_dim):\n",
    "        self.latent_dim = 32\n",
    "        self.input_dim = input_dim\n",
    "        self.n_critic = 5\n",
    "        self.batch_size = 256\n",
    "        optimizer = Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "        \n",
    "        self.generator.trainable = False\n",
    "        \n",
    "        real_data = Input(shape = (self.input_dim,))\n",
    "        z_disc = Input(shape = (self.latent_dim,))\n",
    "        hard_weight = Input(shape = (1,))\n",
    "        \n",
    "        fake_data = self.generator(z_disc)\n",
    "        \n",
    "        disc_real = self.critic(real_data) * hard_weight\n",
    "        disc_fake = self.critic(fake_data) * hard_weight\n",
    "        \n",
    "        alpha = K.random_uniform([self.batch_size,1], 0, 1)\n",
    "        interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "        disc_interpolates = self.critic(interpolates)\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,averaged_samples=interpolates,hard_weight_=hard_weight)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "        \n",
    "        self.critic_model = Model(inputs=[real_data,z_disc,hard_weight],\n",
    "                                  outputs=[disc_real,disc_fake,disc_interpolates])\n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                        self.wasserstein_loss,\n",
    "                                        partial_gp_loss],optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        \n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "        \n",
    "        z_gen = Input(shape=(self.latent_dim,))\n",
    "        generate_data = self.generator(z_gen)\n",
    "        disc_generate = self.critic(generate_data)\n",
    "        self.generator_model = Model(z_gen,disc_generate)\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "        \n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples,hard_weight_):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(hard_weight_ * gradient_penalty)\n",
    "    \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(512,activation=\"relu\",input_dim=self.latent_dim))\n",
    "        model.add(Dense(512,activation=\"relu\"))\n",
    "        model.add(Dense(512,activation=\"relu\"))\n",
    "        model.add(Dense(input_dim))\n",
    "        model.add(LeakyReLU())\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        output = model(noise)\n",
    "        \n",
    "        return Model(noise,output)\n",
    "    \n",
    "    def build_critic(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128,activation=\"relu\",input_dim=self.input_dim))\n",
    "        model.add(Dense(64,activation=\"relu\"))\n",
    "        model.add(Dense(1,activation=\"linear\"))\n",
    "        model.summary()\n",
    "        \n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        output = model(inputs)\n",
    "        \n",
    "        return Model(inputs,output)\n",
    "    \n",
    "    def train(self, epochs, batch_size, weight_model,x):\n",
    "        x_scores = weight_model.predict(x)\n",
    "        gen = inf_hard_normal_weight_train_gen(batch_size, x_scores, x)\n",
    "        real_label = -np.ones((batch_size,1))\n",
    "        fake_label = np.ones((batch_size,1))\n",
    "        dummy = np.zeros((batch_size,1))\n",
    "        for epoch in range(epochs):\n",
    "            for _ in range(self.n_critic):\n",
    "                _data,_weight = gen.__next__()\n",
    "                _noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                disc_loss = self.critic_model.train_on_batch([_data, _noise, _weight],\n",
    "                                                                [real_label, fake_label, dummy])\n",
    "            \n",
    "            gen_loss = self.generator_model.train_on_batch(_noise, real_label)\n",
    "        print(disc_loss, gen_loss)\n",
    "#         print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, disc_loss[0], gen_loss))\n",
    "        return disc_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train_normal.shape[1:]\n",
    "epoch_learner = 30\n",
    "epoch_gen = 1000\n",
    "batch_size = 128\n",
    "nb_batch = int(len(x_train_normal)/batch_size)\n",
    "lr = 0.0001\n",
    "required_improvements = 2 ## early stopping\n",
    "saved_filpath = 'saved_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "for run in np.arange(1):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    \n",
    "    model = anomaly_score_learner(input_shape, lr)\n",
    "    wgan = WGANGP(input_shape)\n",
    "    \n",
    "    gen_normal = inf_normal_train_gen(batch_size,x_train_normal)\n",
    "    gen_vandal = inf_vandal_train_gen(batch_size,x_train_vandal)\n",
    "    \n",
    "    best_cost = np.iinfo(np.int32).max\n",
    "    \n",
    "    for i in range(epoch_learner):\n",
    "        \n",
    "        print(str(i)+'*'*20)\n",
    "        if i!=0:\n",
    "            disc_loss = np.abs(wgan.train(epochs = epoch_gen, batch_size = 256, weight_model = model, x = x_train_normal))\n",
    "            generator_quality = np.exp(- disc_loss)            \n",
    "            \n",
    "        avg_cost = 0  \n",
    "        for j in range(nb_batch):\n",
    "            normal_data_batch = gen_normal.__next__()\n",
    "            normal_data_label = np.zeros((batch_size,1))\n",
    "            normal_data_weight = np.ones(batch_size)\n",
    "            vandal_data_batch = gen_vandal.__next__()\n",
    "            vandal_data_label = np.ones((batch_size,1))\n",
    "            vandal_data_weight = np.ones(batch_size)\n",
    "            \n",
    "            if i == 0: ###修改了这里\n",
    "                data_batch = np.vstack((normal_data_batch,vandal_data_batch))\n",
    "                data_label = np.vstack((normal_data_label,vandal_data_label))\n",
    "                data_weight = np.hstack((normal_data_weight,vandal_data_weight))\n",
    "                \n",
    "            else:\n",
    "                _noise = np.random.normal(0, 1, (int(batch_size), 32))\n",
    "                fake_data_batch = wgan.generator.predict(_noise)\n",
    "                fake_data_label = np.zeros((int(batch_size),1))\n",
    "                fake_data_weight =  generator_quality * np.ones(int(batch_size))\n",
    "                data_batch = np.vstack((normal_data_batch,vandal_data_batch,fake_data_batch))\n",
    "                data_label = np.vstack((normal_data_label,vandal_data_label,fake_data_label))\n",
    "                data_weight = np.hstack((normal_data_weight,vandal_data_weight,fake_data_weight))\n",
    "            \n",
    "            data_batch, data_label,data_weight = sample_shuffle_uspv(data_batch,data_label,data_weight)\n",
    "            #给样本加入权重\n",
    "            d_loss_train = model.train_on_batch(data_batch, data_label,sample_weight = data_weight)\n",
    "            \n",
    "            avg_cost += d_loss_train\n",
    "        \n",
    "        print(\"*****测试集********\")\n",
    "        scores = model.predict(x_test)\n",
    "        precision, recall, threshold = precision_recall_curve(y_test, scores)\n",
    "        pr_auc = auc(recall, precision) \n",
    "        print(roc_auc_score(y_test,scores),pr_auc)\n",
    "        \n",
    "        \n",
    "        if avg_cost < best_cost:\n",
    "                model.save(saved_filpath)\n",
    "                res = [roc_auc_score(y_test,scores),pr_auc]\n",
    "                best_cost = avg_cost\n",
    "                last_improvements = 0\n",
    "            else:\n",
    "                last_improvements += 1 \n",
    "\n",
    "            if last_improvements >= required_improvements:\n",
    "                break;\n",
    "    \n",
    "    print(\"final_result:\" + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
